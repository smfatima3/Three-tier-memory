{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebArena Evaluation - Open Source Models\n",
    "\n",
    "Evaluation of open-source LLMs vs LCA on WebArena tasks:\n",
    "- **Llama 3.1 8B Instruct**\n",
    "- **Gemma 2 9B Instruct**\n",
    "- **Qwen 2.5 7B Instruct**\n",
    "- **Phi-3 Mini 4K**\n",
    "- **LCA** (Multi-agent coordination)\n",
    "\n",
    "## Setup Instructions\n",
    "\n",
    "1. **Runtime**: Set to GPU (Runtime → Change runtime type → GPU)\n",
    "2. **Upload**: Upload `webarena_task.json` to Colab files\n",
    "3. **Run**: Execute cells in order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q transformers accelerate bitsandbytes selenium pandas scipy torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Chrome for browser automation\n",
    "!apt-get update\n",
    "!apt-get install -y chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "print(\"✓ Chrome installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload webarena_task.json\n",
    "from google.colab import files\n",
    "print(\"Please upload webarena_task.json:\")\n",
    "uploaded = files.upload()\n",
    "print(f\"✓ Uploaded: {list(uploaded.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the evaluation script\n",
    "!wget -q https://raw.githubusercontent.com/YOUR_USERNAME/Three-tier-memory/main/webarena_evaluation_opensource.py\n",
    "print(\"✓ Script downloaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative: Paste Script Directly\n",
    "\n",
    "If you can't download from GitHub, copy the entire `webarena_evaluation_opensource.py` content here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "!python webarena_evaluation_opensource.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Options\n",
    "\n",
    "Edit these in the script before running:\n",
    "\n",
    "```python\n",
    "N_TRIALS = 10        # Trials per task (10 for full evaluation)\n",
    "MAX_TASKS = 50       # Number of tasks (50 for paper)\n",
    "n_agents = 5         # LCA agents (5 for paper)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Results\n",
    "\n",
    "After evaluation completes, download the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download results\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "results_dir = 'webarena_results'\n",
    "if os.path.exists(results_dir):\n",
    "    for filename in os.listdir(results_dir):\n",
    "        filepath = os.path.join(results_dir, filename)\n",
    "        print(f\"Downloading {filename}...\")\n",
    "        files.download(filepath)\n",
    "    print(\"✓ All results downloaded\")\n",
    "else:\n",
    "    print(\"❌ No results directory found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Analysis\n",
    "\n",
    "View results summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Find latest CSV\n",
    "csv_files = glob.glob('webarena_results/webarena_opensource_*.csv')\n",
    "if csv_files:\n",
    "    latest_csv = max(csv_files)\n",
    "    df = pd.read_csv(latest_csv)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RESULTS SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    summary = df.groupby('agent').agg({\n",
    "        'success': ['mean', 'std', 'count'],\n",
    "        'time': ['mean', 'std'],\n",
    "        'quality': ['mean', 'std']\n",
    "    }).round(3)\n",
    "    \n",
    "    print(summary)\n",
    "    \n",
    "    # Plot success rates\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    agents = df['agent'].unique()\n",
    "    success_rates = [df[df['agent'] == agent]['success'].mean() for agent in agents]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(agents, success_rates)\n",
    "    plt.xlabel('Agent')\n",
    "    plt.ylabel('Success Rate')\n",
    "    plt.title('WebArena Success Rates by Agent')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"❌ No results found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Out of Memory\n",
    "- Use fewer agents: `n_agents=3` instead of 5\n",
    "- Reduce trials: `N_TRIALS=5` instead of 10\n",
    "- Run models one at a time\n",
    "\n",
    "### Chrome Driver Issues\n",
    "```bash\n",
    "!apt-get update\n",
    "!apt-get install -y chromium-chromedriver\n",
    "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
    "```\n",
    "\n",
    "### Model Loading Fails\n",
    "- Check Hugging Face Hub status\n",
    "- Try different model (some require access approval)\n",
    "- Ensure GPU runtime is enabled"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
